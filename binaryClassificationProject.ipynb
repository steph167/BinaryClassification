{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading csv files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Imports the csv file which contains the data of the training and testing photos\n",
    "data_train = pd.read_csv(\"/kaggle/input/brighton-a-memorable-city/training.csv\")\n",
    "data_test = pd.read_csv(\"/kaggle/input/brighton-a-memorable-city/testing.csv\")\n",
    "confidence = pd.read_csv(\"/kaggle/input/brighton-a-memorable-city/annotation_confidence.csv\")\n",
    "proportion = pd.read_csv(\"/kaggle/input/brighton-a-memorable-city/test_proportions.csv\")\n",
    "validSubmission = pd.read_csv(\"/kaggle/input/brighton-a-memorable-city/sample_valid_submission.csv\")\n",
    "additional_train = pd.read_csv(\"/kaggle/input/brighton-a-memorable-city/additional_training.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all used libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used for the preprocessing and feature selection\n",
    "from sklearn.model_selection import train_test_split # Using Skicit-learn to split data into training and testing sets\n",
    "from sklearn.impute import SimpleImputer # Used when filling in missing details in the additional training data\n",
    "from sklearn.feature_selection import chi2 # Feature selection using chi-squared statistics\n",
    "from sklearn.feature_selection import SelectKBest # Feature selection using chi-squared statistics\n",
    "from imblearn.under_sampling import RandomUnderSampler # Used to balance the training data \n",
    "from imblearn.over_sampling import RandomOverSampler # Used to balance the training data \n",
    "from collections import Counter # To see class distribution\n",
    "\n",
    "# Classifier libraries\n",
    "from sklearn.neural_network import MLPClassifier #Multi-layer-perceptron classifier\n",
    "from sklearn.ensemble import RandomForestClassifier # Random-forest classifier\n",
    "from sklearn.linear_model import LogisticRegression # Logistic-regression classifier\n",
    "from sklearn import neighbors # kNN classifier\n",
    "from sklearn.svm import SVC # Support vector classifier\n",
    "from sklearn.linear_model import SGDClassifier # Supervised gradient descent classifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score # to find the accuracy of the model\n",
    "\n",
    "#Used to download the csv file onto machine\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import base64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the features and labels used\n",
    "\n",
    "Retreving the additional data, only focusing on the CNN features (first 4097), estimating the missing values with the mean of the column. Retrieving the labelled data, only focusing on the CNN features (first 4097), and concatenating them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating the additional training data into CNN and GIST(not using the GIST features)\n",
    "addfeatures = additional_train.iloc[:, :-1].values\n",
    "addfeatures = addfeatures[:, 0:4097]\n",
    "\n",
    "# Seperating the labels of the additional data\n",
    "addLabels = additional_train.iloc[:, -1].values\n",
    "addLabels = addLabels.reshape(-1,1)\n",
    "\n",
    "# Creating an imputer to estimate the missing values using the mean of each column\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "addfeatures = imp.fit_transform(addfeatures)\n",
    "addLabels = imp.fit_transform(addLabels)\n",
    "addLabels = addLabels.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the fully labelled data from data_train, split into features and labels\n",
    "features = data_train.iloc[:, :-1].values\n",
    "labels = data_train.iloc[:, -1].values\n",
    "\n",
    "#Only focusing on the CNN features\n",
    "features = features[:,0:4097]\n",
    "\n",
    "# Add the additional and training data into 2 lists, one for features, one for labels\n",
    "features = np.concatenate((features, addfeatures))\n",
    "labels = np.concatenate((labels, addLabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance the class distribution\n",
    "\n",
    "Use both oversampling and undersampling to balance the class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Old class distriution:', Counter(labels)) \n",
    "\n",
    "# oversampling strategy\n",
    "over = RandomOverSampler(sampling_strategy=0.2)\n",
    "# fit and apply the transform\n",
    "X, y = over.fit_resample(features, labels)\n",
    "\n",
    "# undersampling strategy\n",
    "under = RandomUnderSampler(sampling_strategy=0.8)\n",
    "# fit and apply the transform\n",
    "features, labels = under.fit_resample(X, y)\n",
    "\n",
    "print('New class distriution:', Counter(labels)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Method - Chi-squared\n",
    "\n",
    "Using the filter to reduce the high dimentionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi = SelectKBest(chi2, k=1000) # Initialises the method, sets k to 1000\n",
    "chi.fit(features, labels) # fits to the training features \n",
    "features = chi.transform(features) # Selects features accoring to the 1000 highest scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection - Train-test split\n",
    "\n",
    "Used to evaluate the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 0)\n",
    "\n",
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', test_features.shape)\n",
    "print('Testing Labels Shape:', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate each classifier\n",
    "\n",
    "Trains and tests each classifier to evaluate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('MLP', MLPClassifier()),\n",
    "    ('RF', RandomForestClassifier()),\n",
    "    ('LR', LogisticRegression()),\n",
    "    ('kNN', neighbors.KNeighborsClassifier()),\n",
    "    ('SV', SVC()),\n",
    "    ('SGD', SGDClassifier()),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Over sampling\n",
    "predictions = []\n",
    "for name, model in models:\n",
    "    clf = model\n",
    "    clf.fit(train_features, train_labels)\n",
    "    predictions.append(clf.predict(test_features)) \n",
    "    accuracy = clf.score(test_features, test_labels)\n",
    "    print(name, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "Transforms the testing features so they are the same as the training features and retrieves the predictions from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data_test.values # Retrieves the features from the test data\n",
    "test = test[:, 0:4097] # Only focuses on the CNN featues\n",
    "test = chi.transform(test) # Perform the chi filter method on the features that has been fitted above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter tune the chosen classifier**\n",
    "\n",
    "Using randomisedSearchCV to retrieve the hyperparmeters:\n",
    "\n",
    "> max_depth = [int(x) for x in np.linspace(10, 110, num = 20)]\n",
    "max_depth.append(None)\n",
    "random_grid = {'n_estimators': [int(x) for x in np.linspace(start = 50, stop = 550, num = 100)],\n",
    "               'max_features': ['auto', 'sqrt'],\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': [2, 5],\n",
    "               'min_samples_leaf': [1, 2, 4]}\n",
    "rf_random = RandomizedSearchCV(RandomForestClassifier(), random_grid, cv = 5, n_jobs = -1)\n",
    "rf_random.fit(train_features, train_labels)\n",
    "print(rf_random.score(test_features, test_labels))\n",
    "print(rf_random.best_params_)\n",
    "\n",
    "The best parameters are used to initialise the classifier then train it on the features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=337, min_samples_split=5, min_samples_leaf=1, max_features='sqrt', max_depth=104)\n",
    "clf.fit(features, labels) \n",
    "testPre = clf.predict(test) #Predict the test data labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn the predictions into a valid submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predics = []\n",
    "for i in testPre:\n",
    "    predics.append(int(i))\n",
    "df = pd.DataFrame(data={'ID':np.arange(1,int(len(testPre)+1)), 'prediction': predics}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a link so the csv file can be downloaded to get a csv file of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_download_link(df, title = \"Download CSV file\", filename = \"predictions.csv\"):  \n",
    "    csv = df.to_csv(index=False)\n",
    "    b64 = base64.b64encode(csv.encode())\n",
    "    payload = b64.decode()\n",
    "    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n",
    "    html = html.format(payload=payload,title=title,filename=filename)\n",
    "    return HTML(html)\n",
    "\n",
    "create_download_link(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
